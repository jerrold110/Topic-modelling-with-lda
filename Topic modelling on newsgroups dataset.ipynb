{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "daa16daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3425e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = datasets.fetch_20newsgroups(subset='train', shuffle=True)\n",
    "newsgroups_test = datasets.fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "281502a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40c48998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43c6dd71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa7fbb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "7532\n"
     ]
    }
   ],
   "source": [
    "print(len(newsgroups_train.data))\n",
    "print(len(newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63cb43",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14891b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# converts document into lowercase tokens\n",
    "from gensim.utils import simple_preprocess\n",
    "# Removes stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "# Removes punctuation\n",
    "from string import punctuation\n",
    "# lemmatizes and stems\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea4a9ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'lerxst',\n",
       " 'wam',\n",
       " 'umd',\n",
       " 'edu',\n",
       " 'where',\n",
       " 'my',\n",
       " 'thing',\n",
       " 'subject',\n",
       " 'what',\n",
       " 'car',\n",
       " 'is',\n",
       " 'this',\n",
       " 'nntp',\n",
       " 'posting',\n",
       " 'host',\n",
       " 'rac',\n",
       " 'wam',\n",
       " 'umd',\n",
       " 'edu',\n",
       " 'organization',\n",
       " 'university',\n",
       " 'of',\n",
       " 'maryland',\n",
       " 'college',\n",
       " 'park',\n",
       " 'lines',\n",
       " 'was',\n",
       " 'wondering',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'out',\n",
       " 'there',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'me',\n",
       " 'on',\n",
       " 'this',\n",
       " 'car',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'other',\n",
       " 'day',\n",
       " 'it',\n",
       " 'was',\n",
       " 'door',\n",
       " 'sports',\n",
       " 'car',\n",
       " 'looked',\n",
       " 'to',\n",
       " 'be',\n",
       " 'from',\n",
       " 'the',\n",
       " 'late',\n",
       " 'early',\n",
       " 'it',\n",
       " 'was',\n",
       " 'called',\n",
       " 'bricklin',\n",
       " 'the',\n",
       " 'doors',\n",
       " 'were',\n",
       " 'really',\n",
       " 'small',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'the',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'was',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'body',\n",
       " 'this',\n",
       " 'is',\n",
       " 'all',\n",
       " 'know',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'can',\n",
       " 'tellme',\n",
       " 'model',\n",
       " 'name',\n",
       " 'engine',\n",
       " 'specs',\n",
       " 'years',\n",
       " 'of',\n",
       " 'production',\n",
       " 'where',\n",
       " 'this',\n",
       " 'car',\n",
       " 'is',\n",
       " 'made',\n",
       " 'history',\n",
       " 'or',\n",
       " 'whatever',\n",
       " 'info',\n",
       " 'you',\n",
       " 'have',\n",
       " 'on',\n",
       " 'this',\n",
       " 'funky',\n",
       " 'looking',\n",
       " 'car',\n",
       " 'please',\n",
       " 'mail',\n",
       " 'thanks',\n",
       " 'il',\n",
       " 'brought',\n",
       " 'to',\n",
       " 'you',\n",
       " 'by',\n",
       " 'your',\n",
       " 'neighborhood',\n",
       " 'lerxst']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.utils.simple_preprocess(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55919700",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if (token not in STOPWORDS) and (len(token)>3) and (token not in punctuation):\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "196d7fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['guykuo', 'carson', 'washington', 'subject', 'clock', 'poll', 'final', 'summari', 'final', 'clock', 'report', 'keyword', 'acceler', 'clock', 'upgrad', 'articl', 'shelley', 'qvfo', 'innc', 'organ', 'univers', 'washington', 'line', 'nntp', 'post', 'host', 'carson', 'washington', 'fair', 'number', 'brave', 'soul', 'upgrad', 'clock', 'oscil', 'share', 'experi', 'poll', 'send', 'brief', 'messag', 'detail', 'experi', 'procedur', 'speed', 'attain', 'rat', 'speed', 'card', 'adapt', 'heat', 'sink', 'hour', 'usag', 'floppi', 'disk', 'function', 'floppi', 'especi', 'request', 'summar', 'day', 'network', 'knowledg', 'base', 'clock', 'upgrad', 'haven', 'answer', 'poll', 'thank', 'guykuo', 'washington']]\n"
     ]
    }
   ],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "    \n",
    "print(processed_docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076443c9",
   "metadata": {},
   "source": [
    "### Create Bag of words and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee5ba56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "# words appearing less than 15 times\n",
    "# words appearing in more than 10% of all documents\n",
    "# Keep and 100000 most frequent words\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab41ca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addit\n",
      "1 bodi\n",
      "2 bring\n",
      "3 bumper\n",
      "4 call\n",
      "5 colleg\n",
      "6 door\n",
      "7 earli\n",
      "8 engin\n",
      "9 enlighten\n",
      "10 histori\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7abb63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "96b135ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "Word 18 (\"rest\") appears 1 time.\n",
      "Word 166 (\"clear\") appears 1 time.\n",
      "Word 336 (\"refer\") appears 1 time.\n",
      "Word 350 (\"true\") appears 1 time.\n",
      "Word 391 (\"technolog\") appears 1 time.\n",
      "Word 437 (\"christian\") appears 1 time.\n",
      "Word 453 (\"exampl\") appears 1 time.\n",
      "Word 476 (\"jew\") appears 1 time.\n",
      "Word 480 (\"lead\") appears 1 time.\n",
      "Word 482 (\"littl\") appears 3 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_x = bow_corpus[20]\n",
    "print(len(bow_doc_x))\n",
    "for i in range(10):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfa799",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "492c8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=8, \n",
    "                                       id2word=dictionary,                                    \n",
    "                                       passes=10,\n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ffa7d930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.006*\"bike\" + 0.006*\"presid\" + 0.005*\"game\" + 0.005*\"team\" + 0.004*\"run\" + 0.004*\"player\" + 0.003*\"play\" + 0.003*\"clinton\" + 0.003*\"pitch\" + 0.003*\"virginia\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.009*\"govern\" + 0.007*\"armenian\" + 0.006*\"israel\" + 0.005*\"kill\" + 0.005*\"isra\" + 0.004*\"american\" + 0.004*\"turkish\" + 0.004*\"countri\" + 0.004*\"weapon\" + 0.004*\"jew\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.015*\"game\" + 0.013*\"team\" + 0.010*\"play\" + 0.008*\"hockey\" + 0.008*\"player\" + 0.005*\"canada\" + 0.005*\"season\" + 0.004*\"leagu\" + 0.004*\"score\" + 0.004*\"andrew\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"card\" + 0.010*\"window\" + 0.007*\"driver\" + 0.006*\"sale\" + 0.005*\"price\" + 0.005*\"appl\" + 0.005*\"speed\" + 0.005*\"video\" + 0.005*\"engin\" + 0.005*\"monitor\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.014*\"file\" + 0.010*\"program\" + 0.009*\"window\" + 0.006*\"encrypt\" + 0.006*\"chip\" + 0.006*\"imag\" + 0.006*\"data\" + 0.006*\"avail\" + 0.005*\"version\" + 0.004*\"code\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.012*\"space\" + 0.009*\"nasa\" + 0.006*\"scienc\" + 0.005*\"orbit\" + 0.005*\"research\" + 0.004*\"launch\" + 0.004*\"pitt\" + 0.003*\"earth\" + 0.003*\"develop\" + 0.003*\"bank\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.033*\"drive\" + 0.015*\"scsi\" + 0.010*\"disk\" + 0.009*\"hard\" + 0.008*\"control\" + 0.007*\"columbia\" + 0.006*\"washington\" + 0.006*\"uiuc\" + 0.005*\"car\" + 0.004*\"jumper\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.012*\"christian\" + 0.008*\"jesus\" + 0.006*\"exist\" + 0.005*\"moral\" + 0.005*\"bibl\" + 0.005*\"word\" + 0.005*\"religion\" + 0.005*\"church\" + 0.004*\"life\" + 0.004*\"claim\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = lda_model.print_topics(num_words=-1)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b09ff",
   "metadata": {},
   "source": [
    "### Classification of topics\n",
    "\n",
    "* 0: Sports\n",
    "* 1: Politics\n",
    "* 2: Hockey\n",
    "* 3: Graphic cards\n",
    "* 4: Computers\n",
    "* 5: Space\n",
    "* 6: Automobiles\n",
    "* 7: Atheism\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79575ed0",
   "metadata": {},
   "source": [
    "Testing an unseen document\n",
    "\n",
    "This document has the topic atheism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c958cc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: mathew <mathew@mantis.co.uk>\n",
      "Subject: Re: STRONG & weak Atheism\n",
      "Organization: Mantis Consultants, Cambridge. UK.\n",
      "X-Newsreader: rusnews v1.02\n",
      "Lines: 9\n",
      "\n",
      "acooper@mac.cc.macalstr.edu (Turin Turambar, ME Department of Utter Misery) writes:\n",
      "> Did that FAQ ever got modified to re-define strong atheists as not those who\n",
      "> assert the nonexistence of God, but as those who assert that they BELIEVE in \n",
      "> the nonexistence of God?\n",
      "\n",
      "In a word, yes.\n",
      "\n",
      "\n",
      "mathew\n",
      "\n",
      "alt.atheism\n"
     ]
    }
   ],
   "source": [
    "num = 2\n",
    "unseen_document = newsgroups_test.data[num]\n",
    "print(unseen_document)\n",
    "\n",
    "print(list(newsgroups_train.target_names)[newsgroups_test.target[num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ca1d2ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0.9634918)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "lda_model[bow_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1fe64d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c35951b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012*\"christian\" + 0.008*\"jesus\" + 0.006*\"exist\" + 0.005*\"moral\" + 0.005*\"bibl\"\n"
     ]
    }
   ],
   "source": [
    "for i in lda_model[bow_vector]:\n",
    "    print(lda_model.print_topic(i[0], 5))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "139325c1",
   "metadata": {},
   "source": [
    "The model correctly classifies the unseen document with 'x'% probability to the X category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
